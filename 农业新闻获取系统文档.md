# 农业新闻获取与处理系统文档

## 系统概述

本系统是一个自动化的农业新闻采集、验证和提取的工具，由三个主要步骤组成，可以自动化地监控农业网站、提取新闻链接、验证链接有效性，并从有效链接中提取内容。整个系统采用流水线处理方式，确保数据的流畅传输和处理。

### 核心流程

1. **步骤1 (step_1_homepage_monitor.py)**: 监控农业网站主页，提取新链接
2. **步骤2 (step_2_link_validator.py)**: 验证链接是否为有效的农业新闻
3. **步骤3 (step_3_content_extraction.py)**: 从有效链接中提取新闻内容

系统通过批次ID（batch_id）机制在各个步骤之间传递数据，确保数据的一致性和可追踪性。

## 技术架构

### 使用的框架与服务

- **Firecrawl API**: 用于网站映射、链接提取和内容抓取
- **通义千问API (DashScope)**: 用于验证链接是否为有效的农业新闻
- **pandas**: 用于数据处理和Excel文件输出
- **logging**: 用于系统日志记录

### 文件结构

- **config.py**: 配置文件，包含API密钥、文件路径、超时设置等
- **step_1_homepage_monitor.py**: 主页监控模块
- **step_2_link_validator.py**: 链接验证模块
- **step_3_content_extraction.py**: 内容提取模块
- **APP.py**: 主应用程序，协调各模块工作

### 数据流

```
主页URL (Excel) → 步骤1 → 新链接 (JSON) → 步骤2 → 有效链接 (JSON) → 步骤3 → 提取内容 (JSON & Excel)
```

## 详细模块说明

### 1. 步骤1：主页监控 (step_1_homepage_monitor.py)

#### 功能概述
监控指定农业网站的主页，使用map_url方法提取页面上的所有链接，并通过与历史记录比较，识别新发布的链接。

#### 核心API

```python
# Firecrawl的map_url方法
result = self.app.map_url(url, params={
    'includeSubdomains': False,  # 包含/排除子域名
    'limit': 500,                # 链接获取上限
    'timeout': 60000             # 超时时间（毫秒）
})
```

#### 处理流程
1. 从Excel文件读取主页URL列表
2. 对每个URL使用map_url方法获取所有链接
3. 过滤无效链接（例如媒体文件、登录页等）
4. 与历史记录比较，找出新链接
5. 为新链接生成批次ID (batch_id)
6. 保存新链接到JSON文件

#### 重要特性
- **批次ID生成**: 基于时间戳自动生成唯一批次ID
- **链接过滤**: 通过排除模式过滤无效链接
- **自引用链接移除**: 排除指向当前页面的链接
- **重试机制**: 处理API速率限制和网络超时

### 2. 步骤2：链接验证 (step_2_link_validator.py)

#### 功能概述
使用通义千问API验证链接是否为有效的农业新闻文章，评估链接的有效性并给出分数。

#### 核心API

```python
# 通义千问API调用
response = Generation.call(
    model=MODEL_ID,
    prompt=prompt,
    temperature=0.1,
    max_tokens=1500,
    top_p=0.8
)
```

#### 处理流程
1. 加载特定批次ID的新链接
2. 对每个链接构造验证提示词
3. 调用通义千问API进行链接验证
4. 解析API返回的JSON响应
5. 保存验证结果到JSON文件

#### 重要特性
- **基于批次ID处理**: 支持指定批次ID或处理最新批次
- **自定义验证标准**: 基于URL路径、链接文本等评估链接有效性
- **智能评分**: 对每个链接给出0-100的有效性评分

### 3. 步骤3：内容提取 (step_3_content_extraction.py)

#### 功能概述
从已验证的有效链接中提取新闻内容，包括标题、正文、发布日期等结构化数据。

#### 核心API

```python
# 提取markdown内容
scrape_result = self.app.scrape_url(
    url, 
    params={'formats': ['markdown']}
)

# 提取结构化数据
json_result = self.app.scrape_url(
    url, 
    params={
        'formats': ['json'],
        'jsonOptions': {
            'schema': schema,
            'prompt': "提取这篇农业新闻文章的完整内容，包括标题、正文、发布日期、来源、分类、作者和摘要。"
        }
    }
)
```

#### 处理流程
1. 根据批次ID或时间戳加载有效链接
2. 对每个链接先提取markdown内容
3. 再提取结构化的JSON数据
4. 保存中间结果（每处理一个链接后）
5. 最终生成完整的JSON和Excel结果

#### 重要特性
- **分步抓取**: 先获取markdown再获取结构化数据，提高成功率
- **中间结果保存**: 每处理一个链接就保存一次，避免数据丢失
- **多格式输出**: 同时输出JSON和Excel格式的结果
- **批量处理暂停**: 每处理N个链接暂停一段时间，避免API限制

### 4. 主应用程序 (APP.py)

#### 功能概述
作为整个系统的控制中心，协调各个模块的工作，支持单步运行或完整流程执行。

#### 处理流程
1. 解析命令行参数
2. 初始化各个模块
3. 根据参数执行指定步骤或完整流程
4. 记录详细日志

#### 重要特性
- **命令行参数支持**: 灵活指定运行步骤、批次ID等
- **日志系统**: 详细记录执行过程和错误信息
- **完整流水线**: 支持一键运行完整处理流程
- **错误处理**: 完善的异常捕获和处理机制

## 系统配置 (config.py)

### 主要配置项

```python
# API密钥
API_KEY = "fc-001ba65834e44931b9dde005c1e6afd4"  # Firecrawl API密钥
DASHSCOPE_API_KEY = "sk-f3a5242b79b64966acdb5810eb5488f3"  # 通义千问API密钥
DASHSCOPE_MODEL_ID = "qwen-plus"  # 使用的模型ID

# 文件路径
HOMEPAGE_EXCEL_PATH = "C:\\Python\\github\\firecrawl\\testhomepage.xlsx"
LINKS_CACHE_FILE = "link_cache.json"
NEW_LINKS_FILE = "new_links.json"
VALID_LINKS_FILE = "new_links_valid.json"

# API请求设置
REQUEST_TIMEOUT = 30  # 请求超时时间（秒）
MAX_RETRIES = 2      # 最大重试次数
RETRY_DELAY = 2      # 重试间隔（秒）
BATCH_PAUSE_TIME = 1  # 批处理暂停时间（秒）
BATCH_SIZE = 5       # 每批处理的大小

# 验证设置
MIN_VALID_SCORE = 70  # 链接验证有效的最低分数
```

## 使用指南

### 安装依赖

```bash
pip install firecrawl-py dashscope pandas openpyxl
```

### 运行方式

#### 完整流程

```bash
python APP.py --all
```

#### 单步运行

```bash
# 只运行步骤1（主页监控）
python APP.py --step 1

# 对指定批次运行步骤2（链接验证）
python APP.py --step 2 --batch batch_20250403114912

# 对指定批次运行步骤3（内容提取）
python APP.py --step 3 --batch batch_20250403114912
```

#### 使用指定批次ID运行完整流程（跳过步骤1）

```bash
python APP.py --all --batch batch_20250403114912
```

### 数据文件说明

- `testhomepage.xlsx`: 包含要监控的主页URL列表
- `link_cache.json`: 存储历史链接记录用于去重
- `new_links.json`: 存储新发现的链接和批次信息
- `new_links_valid.json`: 存储验证后的有效链接
- `extracted_content_*`: 提取的内容结果目录

## 系统优化建议

1. **网络稳定性增强**:
   - 增加更智能的重试机制
   - 考虑使用代理服务器处理地域限制

2. **批处理优化**:
   - 调整BATCH_SIZE和BATCH_PAUSE_TIME以平衡速度和稳定性

3. **内容处理改进**:
   - 增加自然语言处理功能，提取新闻关键词和主题
   - 添加新闻分类和聚类功能

4. **数据存储升级**:
   - 考虑使用数据库替代JSON文件存储
   - 添加数据压缩和备份机制

5. **UI界面开发**:
   - 开发Web界面方便监控和管理系统运行
   - 添加数据可视化和报表功能

## 故障排除

### 常见问题

1. **API速率限制**:
   - 错误信息: "Rate limit exceeded"
   - 解决方案: 增加BATCH_PAUSE_TIME或减少BATCH_SIZE

2. **网络超时**:
   - 错误信息: "Request Timeout"
   - 解决方案: 检查网络连接，增加timeout参数

3. **API响应解析失败**:
   - 错误信息: "JSON解析错误"
   - 解决方案: 检查API响应格式，调整解析逻辑

4. **文件访问错误**:
   - 错误信息: "加载文件时出错"
   - 解决方案: 检查文件路径和权限

### 日志分析

系统日志保存在`logs`目录下，日志文件名格式为`agrinews_YYYYMMDD.log`。通过分析日志可以定位大多数问题的根源。

## 结语

农业新闻获取与处理系统提供了一套完整的解决方案，用于自动化地监控、验证和提取农业新闻。系统模块化设计使得维护和扩展变得简单，批次ID机制保证了数据的一致性和可追踪性。

系统还具有良好的错误处理和重试机制，能够应对各种网络和API限制问题。通过简单的命令行参数，用户可以灵活地控制系统的运行方式和处理流程。 